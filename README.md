# PARALLELIZED-IMAGE-CRAWLING-USING-MULTITHREADING

## ABSTRACT ##
In this project, I design a parallel web crawler by implementing multithreading concept. Web crawlers are used by search engines for indexing of webpages. A web crawler bot is like someone who goes through all the books in a disorganized library and puts together a card catalogue so that anyone who visits the library can quickly and easily find the information they need. To help categorize and sort the library's books by topic, the organizer will read the title, summary, and some of the internal text of each book to figure out what it's about. The goal of our proposed bot is to crawl websites for images of a particular keyword and automating the download process for matching images. Multi-threading is used to speed up the crawling as the internet has a vast scope and it would be inefficient to use a single process to crawl the internet. The project will help in downloading images of keywords that can be given to the program and the web crawler will scrape google images for those particular keywords and download the images and place them in separate folders based on keywords. The downloaded images maybe used by the user as per their choice.

## INTRODUCTION ##
A web crawler is considered to be a search engine bot or a spider that downloads and indexes content from all over the Internet. The goal of such a bot is to learn what (almost) every webpage on the web is about, so that the information can be retrieved when it's needed. They're called web crawlers because crawling is the technical term for automatically accessing a website and obtaining data via a software program. These bots are almost always operated by search engines. By applying a search algorithm to the data collected by web crawlers, search engines can provide relevant links inresponse to user search queries, generating the list of webpages that show up after a user types a search into Google or Bing (or another search engine). A web crawler bot is like someone who goes through all the books in a disorganized library and puts together a card catalog so that anyone who visits the library can quickly and easily find the information they need. To help categorize and sort the library's books by topic, the organizer will read the title, summary, and some of the internal text of each book to figure out what it's about. A crawler is a program that downloads and stores Web pages, often for a Web search engine. Roughly, a crawler starts off by placing an initial set of URLs, in a queue, where all URLs to be retrieved are kept and prioritized. From this queue, the crawler gets a URL (in some order), downloads the page, extracts any URLs in the downloaded page, and puts the new URLs in the queue. This process is repeated until the crawler decides to stop. Collected pages are later used for other applications, such as a Web search engine or a Web cache. As the size of the Web grows, it becomes more difficult to retrieve the whole or a significant portion of the Web using a single process. Therefore, many search engines often run multiple processes in parallel to perform the above task, so that download rate is maximized. We refer to this type of crawler as a parallel crawler
A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. In this project we have created a web crawler that automates searching of the keywords that is, it scrapes the web for the keywords that have been provided by the user on Google Images and provides a multi-threading solution for the download of the images that have been scraped. Parallel web crawlers can be beneficial as compared to normal web crawlers in many scenarios such as: collecting data from multiple sources (multiple remote servers) instead of just a single source, performing long/complex operations on the collected data (such as doing image analysis or OCR) that could be done in parallel with fetching the data and collecting data from a large web service where you are paying for each query, or where creating multiple connections to the service is within the bounds of your usage agreement among other benefits. Parallel Web Crawlers also have great benefits when it comes to scalability, handling network loads and others. Parallelization is an extremely efficient optimization that can be implemented to a web crawler and that is the objective of our application.
